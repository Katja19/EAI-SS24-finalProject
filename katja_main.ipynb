{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAI Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of zenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ Initializing ZenML repository at \n",
      "c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject.\n",
      "\n",
      "⠋ Initializing ZenML repository at \n",
      "c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject.\n",
      "\n",
      "⠹ Initializing ZenML repository at \n",
      "c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject.\n",
      "\n",
      "⠼ Initializing ZenML repository at \n",
      "c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject.\n",
      "\u001b[1;35mSetting the repo active workspace to 'default'.\u001b[0m\n",
      "\u001b[33mSetting the repo active stack to default.\u001b[0m\n",
      "\n",
      "ZenML repository initialized at \n",
      "c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject.\n",
      "⠼ Initializing ZenML repository at \n",
      "c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject.\n",
      "\n",
      "⠼ Initializing ZenML repository at \n",
      "c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject.\n",
      "\n",
      "\n",
      "The local active stack was initialized to 'default'. This local configuration \n",
      "will only take effect when you're running ZenML from the initialized repository\n",
      "root, or from a subdirectory. For more information on repositories and \n",
      "configurations, please visit \n",
      "https://docs.zenml.io/user-guide/starter-guide/understand-stacks.\n",
      "⠋ Installing integrations...\n",
      "⠙ Installing integrations...\n",
      "⠹ Installing integrations...\n",
      "⠸ Installing integrations...\n",
      "⠼ Installing integrations...\n",
      "⠴ Installing integrations...\n",
      "⠦ Installing integrations...\n",
      "⠧ Installing integrations...\n",
      "⠇ Installing integrations...\n",
      "⠏ Installing integrations...\n",
      "⠙ Installing integrations...\n",
      "⠹ Installing integrations...\n",
      "⠸ Installing integrations...\n",
      "⠼ Installing integrations...\n",
      "⠦ Installing integrations...\n",
      "⠧ Installing integrations...\n",
      "⠇ Installing integrations...\n",
      "⠏ Installing integrations...\n",
      "⠋ Installing integrations...\n",
      "⠙ Installing integrations...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we are going to create a new ZenML project\n",
    "#!rm -rf .zen # don't work under windows, cause it's a unix-command\n",
    "!rmdir /S /Q .zen\n",
    "!zenml init\n",
    "!zenml integration install sklearn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Stack Configuration        \n",
      "┌────────────────┬────────────────┐\n",
      "│ COMPONENT_TYPE │ COMPONENT_NAME │\n",
      "├────────────────┼────────────────┤\n",
      "│ ARTIFACT_STORE │ default        │\n",
      "├────────────────┼────────────────┤\n",
      "│ ORCHESTRATOR   │ default        │\n",
      "└────────────────┴────────────────┘\n",
      "     'default' stack (ACTIVE)      \n",
      "⠋ Describing the stack...\n",
      "\n",
      "No labels are set for this stack.\n",
      "⠋ Describing the stack...\n",
      "\n",
      "Stack 'default' with id '55222316-9ac8-47bc-97de-829f73a8a621' is unowned.\n",
      "⠋ Describing the stack...\n",
      "\n",
      "⠋ Describing the stack...\n",
      "\n",
      "\n",
      "You can display various ZenML entities including pipelines, runs, stacks and \n",
      "much more on the ZenML Dashboard. You can try it locally, by running `zenml \n",
      "up`, or remotely, by deploying ZenML on the infrastructure of your choice.\n"
     ]
    }
   ],
   "source": [
    "# First we are checking the status of the stack\n",
    "!zenml stack describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ Installing integrations...\n",
      "⠙ Installing integrations...\n",
      "⠹ Installing integrations...\n",
      "⠸ Installing integrations...\n",
      "⠼ Installing integrations...\n",
      "⠴ Installing integrations...\n",
      "⠧ Installing integrations...\n",
      "⠇ Installing integrations...\n",
      "⠏ Installing integrations...\n",
      "⠋ Installing integrations...\n",
      "⠙ Installing integrations...\n",
      "⠸ Installing integrations...\n",
      "⠼ Installing integrations...\n",
      "⠴ Installing integrations...\n",
      "⠦ Installing integrations...\n",
      "⠧ Installing integrations...\n",
      "⠇ Installing integrations...\n",
      "⠋ Installing integrations...\n",
      "⠙ Installing integrations...\n",
      "⠙ Installing integrations...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to install the MLFlow integration, which is a very popular open-source platform for managing the end-to-end machine learning lifecycle\n",
    "#!zenml integration install mlflow -y\n",
    "!zenml integration install wandb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠙ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠹ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠸ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠼ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠦ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠧ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠇ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠏ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠋ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "⠙ Registering experiment tracker 'wandb_experiment_tracker'...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌───────────────────── Traceback (most recent call last) ─────────────────────┐\n",
      "│ in _run_module_as_main:198                                                  │\n",
      "│ in _run_code:88                                                             │\n",
      "│                                                                             │\n",
      "│ in <module>:7                                                               │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\click\\core.py: │\n",
      "│ 1130 in __call__                                                            │\n",
      "│                                                                             │\n",
      "│   1127 │                                                                    │\n",
      "│   1128 │   def __call__(self, *args: t.Any, **kwargs: t.Any) -> t.Any:      │\n",
      "│   1129 │   │   \"\"\"Alias for :meth:`main`.\"\"\"                                │\n",
      "│ > 1130 │   │   return self.main(*args, **kwargs)                            │\n",
      "│   1131                                                                      │\n",
      "│   1132                                                                      │\n",
      "│   1133 class Command(BaseCommand):                                          │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\click\\core.py: │\n",
      "│ 1055 in main                                                                │\n",
      "│                                                                             │\n",
      "│   1052 │   │   try:                                                         │\n",
      "│   1053 │   │   │   try:                                                     │\n",
      "│   1054 │   │   │   │   with self.make_context(prog_name, args, **extra) as  │\n",
      "│ > 1055 │   │   │   │   │   rv = self.invoke(ctx)                            │\n",
      "│   1056 │   │   │   │   │   if not standalone_mode:                          │\n",
      "│   1057 │   │   │   │   │   │   return rv                                    │\n",
      "│   1058 │   │   │   │   │   # it's not safe to `ctx.exit(rv)` here!          │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\click\\core.py: │\n",
      "│ 1657 in invoke                                                              │\n",
      "│                                                                             │\n",
      "│   1654 │   │   │   │   super().invoke(ctx)                                  │\n",
      "│   1655 │   │   │   │   sub_ctx = cmd.make_context(cmd_name, args, parent=ct │\n",
      "│   1656 │   │   │   │   with sub_ctx:                                        │\n",
      "│ > 1657 │   │   │   │   │   return _process_result(sub_ctx.command.invoke(su │\n",
      "│   1658 │   │                                                                │\n",
      "│   1659 │   │   # In chain mode we create the contexts step by step, but aft │\n",
      "│   1660 │   │   # base command has been invoked.  Because at that point we d │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\click\\core.py: │\n",
      "│ 1657 in invoke                                                              │\n",
      "│                                                                             │\n",
      "│   1654 │   │   │   │   super().invoke(ctx)                                  │\n",
      "│   1655 │   │   │   │   sub_ctx = cmd.make_context(cmd_name, args, parent=ct │\n",
      "│   1656 │   │   │   │   with sub_ctx:                                        │\n",
      "│ > 1657 │   │   │   │   │   return _process_result(sub_ctx.command.invoke(su │\n",
      "│   1658 │   │                                                                │\n",
      "│   1659 │   │   # In chain mode we create the contexts step by step, but aft │\n",
      "│   1660 │   │   # base command has been invoked.  Because at that point we d │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\click\\core.py: │\n",
      "│ 1404 in invoke                                                              │\n",
      "│                                                                             │\n",
      "│   1401 │   │   │   echo(style(message, fg=\"red\"), err=True)                 │\n",
      "│   1402 │   │                                                                │\n",
      "│   1403 │   │   if self.callback is not None:                                │\n",
      "│ > 1404 │   │   │   return ctx.invoke(self.callback, **ctx.params)           │\n",
      "│   1405 │                                                                    │\n",
      "│   1406 │   def shell_complete(self, ctx: Context, incomplete: str) -> t.Lis │\n",
      "│   1407 │   │   \"\"\"Return a list of completions for the incomplete value. Lo │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\click\\core.py: │\n",
      "│ 760 in invoke                                                               │\n",
      "│                                                                             │\n",
      "│    757 │   │                                                                │\n",
      "│    758 │   │   with augment_usage_errors(__self):                           │\n",
      "│    759 │   │   │   with ctx:                                                │\n",
      "│ >  760 │   │   │   │   return __callback(*args, **kwargs)                   │\n",
      "│    761 │                                                                    │\n",
      "│    762 │   def forward(                                                     │\n",
      "│    763 │   │   __self, __cmd: \"Command\", *args: t.Any, **kwargs: t.Any  # n │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\cli\\stac │\n",
      "│ k_components.py:287 in register_stack_component_command                     │\n",
      "│                                                                             │\n",
      "│    284 │   │                                                                │\n",
      "│    285 │   │   with console.status(f\"Registering {display_name} '{name}'... │\n",
      "│    286 │   │   │   # Create a new stack component model                     │\n",
      "│ >  287 │   │   │   component = client.create_stack_component(               │\n",
      "│    288 │   │   │   │   name=name,                                           │\n",
      "│    289 │   │   │   │   flavor=flavor,                                       │\n",
      "│    290 │   │   │   │   component_type=component_type,                       │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\client_l │\n",
      "│ azy_loader.py:198 in _inner                                                 │\n",
      "│                                                                             │\n",
      "│   195 │   │   │   │   │   with contextlib.suppress(ValueError):             │\n",
      "│   196 │   │   │   │   │   │   kwargs[k] = ClientLazyLoader(**v).evaluate()  │\n",
      "│   197 │   │   │                                                             │\n",
      "│ > 198 │   │   │   return func(*args_, **kwargs)                             │\n",
      "│   199 │   │                                                                 │\n",
      "│   200 │   │   return _inner                                                 │\n",
      "│   201                                                                       │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\client.p │\n",
      "│ y:2003 in create_stack_component                                            │\n",
      "│                                                                             │\n",
      "│   2000 │   │   )                                                            │\n",
      "│   2001 │   │                                                                │\n",
      "│   2002 │   │   # Register the new model                                     │\n",
      "│ > 2003 │   │   return self.zen_store.create_stack_component(                │\n",
      "│   2004 │   │   │   component=create_component_model                         │\n",
      "│   2005 │   │   )                                                            │\n",
      "│   2006                                                                      │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\analytic │\n",
      "│ s\\utils.py:178 in inner_func                                                │\n",
      "│                                                                             │\n",
      "│   175 │   │   │   │   except Exception as e:                                │\n",
      "│   176 │   │   │   │   │   logger.debug(f\"Analytics tracking failure for {fu │\n",
      "│   177 │   │   │   │                                                         │\n",
      "│ > 178 │   │   │   │   result = func(*args, **kwargs)                        │\n",
      "│   179 │   │   │   │                                                         │\n",
      "│   180 │   │   │   │   try:                                                  │\n",
      "│   181 │   │   │   │   │   if isinstance(result, AnalyticsTrackedModelMixin) │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\zen_stor │\n",
      "│ es\\sql_zen_store.py:3103 in create_stack_component                          │\n",
      "│                                                                             │\n",
      "│    3100 │   │   \"\"\"                                                         │\n",
      "│    3101 │   │   validate_name(component)                                    │\n",
      "│    3102 │   │   with Session(self.engine) as session:                       │\n",
      "│ >  3103 │   │   │   self._fail_if_component_with_name_type_exists(          │\n",
      "│    3104 │   │   │   │   name=component.name,                                │\n",
      "│    3105 │   │   │   │   component_type=component.type,                      │\n",
      "│    3106 │   │   │   │   workspace_id=component.workspace,                   │\n",
      "│                                                                             │\n",
      "│ C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\zen_stor │\n",
      "│ es\\sql_zen_store.py:3384 in _fail_if_component_with_name_type_exists        │\n",
      "│                                                                             │\n",
      "│    3381 │   │   │   .where(StackComponentSchema.type == component_type)     │\n",
      "│    3382 │   │   ).first()                                                   │\n",
      "│    3383 │   │   if existing_domain_component is not None:                   │\n",
      "│ >  3384 │   │   │   raise StackComponentExistsError(                        │\n",
      "│    3385 │   │   │   │   f\"Unable to register '{component_type}' component \" │\n",
      "│    3386 │   │   │   │   f\"with name '{name}': Found an existing \"           │\n",
      "│    3387 │   │   │   │   f\"component with the same name and type in the same │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "StackComponentExistsError: Unable to register 'experiment_tracker' component \n",
      "with name 'wandb_experiment_tracker': Found an existing component with the same\n",
      "name and type in the same  workspace 'default'.\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to register the MLFlow integration for the experiment tracker, which is the component that tracks the experiments and logs the results\n",
    "#!zenml experiment-tracker register mlflow_experiment_tracker --flavor=mlflow\n",
    "#api_key_wandb = \"c29a906eb81fb84dcd21e07090209c1ac6e7b6a4\"\n",
    "import os\n",
    "api_key_wandb = os.environ['WANDB_API_KEY']\n",
    "!zenml experiment-tracker register wandb_experiment_tracker --flavor=wandb --api_key=$api_key_wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: A stack with name `wandb_stack` already exists, please use a different name.\n"
     ]
    }
   ],
   "source": [
    "!zenml stack register wandb_stack -a default -o default -e wandb_experiment_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to register the MLFlow integration for the model deployer, which is the component that deploys the models\n",
    "#!zenml model-deployer register mlflow_deployer --flavor=mlflow\n",
    "#!zenml model-deployer register wandb_deployer --flavor=wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hint__: if the stack does not show the mlflow integration, check in the terminal if the mlflow is installed with  \n",
    "- zenml experiment-tracker list  \n",
    "- zenml model-deployer list  \n",
    "- zenml stack list  \n",
    "- activate the mlflow with: zenml stack set mlflow_stack  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zenml stack register wandb_stack -e wandb_experiment_tracker -m wandb_deployer -a default -o default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active repository stack set to: 'wandb_stack'\n",
      "⠋ Setting the repository active stack to 'wandb_stack'...\n",
      "⠋ Setting the repository active stack to 'wandb_stack'...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change the stack to the mlflow stack\n",
    "#!zenml stack set mlflow_stack\n",
    "!zenml stack set wandb_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Stack Configuration               \n",
      "┌────────────────────┬──────────────────────────┐\n",
      "│ COMPONENT_TYPE     │ COMPONENT_NAME           │\n",
      "├────────────────────┼──────────────────────────┤\n",
      "│ ARTIFACT_STORE     │ default                  │\n",
      "├────────────────────┼──────────────────────────┤\n",
      "│ EXPERIMENT_TRACKER │ wandb_experiment_tracker │\n",
      "├────────────────────┼──────────────────────────┤\n",
      "│ ORCHESTRATOR       │ default                  │\n",
      "└────────────────────┴──────────────────────────┘\n",
      "          'wandb_stack' stack (ACTIVE)           \n",
      "⠋ Describing the stack...\n",
      "\n",
      "           Labels           \n",
      "┌──────────────────┬───────┐\n",
      "│ LABEL            │ VALUE │\n",
      "├──────────────────┼───────┤\n",
      "│ zenml:full_stack │ True  │\n",
      "└──────────────────┴───────┘\n",
      "⠋ Describing the stack...\n",
      "\n",
      "Stack 'wandb_stack' with id '10466513-5196-4c4e-9403-e0334a3670a0' is owned by \n",
      "user default.\n",
      "⠋ Describing the stack...\n",
      "\n",
      "⠋ Describing the stack...\n",
      "\n",
      "\n",
      "You can display various ZenML entities including pipelines, runs, stacks and \n",
      "much more on the ZenML Dashboard. You can try it locally, by running `zenml \n",
      "up`, or remotely, by deploying ZenML on the infrastructure of your choice.\n"
     ]
    }
   ],
   "source": [
    "# At last we are going to check the status of the stack again\n",
    "#!zenml stack set mlflow_stack\n",
    "!zenml stack describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including wandb as alternative to mlflow_experiment_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zenml integration install wandb -y\n",
    "#!zenml experiment-tracker register wandb_experiment_tracker --flavor=wandb\n",
    "#!zenml stack update mlflow_stack -e wandb_experiment_tracker\n",
    "#!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify your pipeline to use wandb for logging:\n",
    "In your ZenML pipeline, you need to ensure that wandb is used to log the models. Here is an example of how you can modify your pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from zenml.steps import step\n",
    "# import wandb\n",
    "\n",
    "# @step\n",
    "# def trainer_step(...) -> ...:\n",
    "#     ...\n",
    "#     # Initialize wandb\n",
    "#     wandb.init(project=\"my_project\", entity=\"my_entity\")\n",
    "    \n",
    "#     # Log model\n",
    "#     wandb.log({\"model\": model})\n",
    "\n",
    "#     # Save model artifact\n",
    "#     wandb.save(\"model.h5\")\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univaraint Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariant Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import run_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStarting the feature engineering pipeline.\u001b[0m\n",
      "\u001b[1;35mStarting feature engineering pipeline...\u001b[0m\n",
      "\u001b[1;35mData updated.\u001b[0m\n",
      "\u001b[1;35mFeature engineering pipeline successfully completed.\u001b[0m\n",
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mfeature_engineering_pipeline\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mReusing registered pipeline version: \u001b[0m\u001b[1;36m(version: 50)\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mExecuting a new run.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mwandb_stack\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  experiment_tracker: \u001b[0m\u001b[1;36mwandb_experiment_tracker\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml up\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mupdate_data\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mupdate_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mload_data\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mload_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mcreate_derived_features\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mcreate_derived_features\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36msplit_data\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36msplit_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mcreate_preprocessing_pipeline\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mcreate_preprocessing_pipeline\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mfeature_preprocessor\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mfeature_preprocessor\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m1.381s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mFeature engineering pipeline completed.\u001b[0m\n",
      "\u001b[1;35mStarting the training pipeline.\u001b[0m\n",
      "\u001b[1;35mStarting training_pipeline...\u001b[0m\n",
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mtraining_pipeline\u001b[1;35m.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject\\pipelines\\training_pipeline.py\", line 46, in training_pipeline\n",
      "    deploy, rmse, mse, r2, mae = evaluate_model(model,X_test,y_test)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\steps\\entrypoint_function_utils.py\", line 113, in __iter__\n",
      "    raise StepInterfaceError(\n",
      "zenml.exceptions.StepInterfaceError: Unable to unpack step artifact. This error is probably because you're trying to unpack the return value of your step but the step only returns a single artifact. For more information on how to add type annotations to your step to indicate multiple artifacts visit https://docs.zenml.io/how-to/build-pipelines/step-output-typing-and-annotation#type-annotations.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\logging\\__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\logging\\__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\logger.py\", line 87, in format\n",
      "    formatted_message = formatter.format(record)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\logging\\__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\logging\\__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_34568\\2733353327.py\", line 7, in <module>\n",
      "    run_pipelines(model_variant=m_variant, model_type=m_type)\n",
      "  File \"c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject\\run.py\", line 32, in run_pipelines\n",
      "    training_pipeline(model_variant=model_variant, model_type=model_type)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\new\\pipelines\\pipeline.py\", line 1409, in __call__\n",
      "    self.prepare(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\new\\pipelines\\pipeline.py\", line 490, in prepare\n",
      "    self._call_entrypoint(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\zenml\\new\\pipelines\\pipeline.py\", line 1438, in _call_entrypoint\n",
      "    self.entrypoint(**validated_args)\n",
      "  File \"c:\\Users\\User\\VSCProjects\\EAI_SS24\\EAI-SS24-finalProject\\pipelines\\training_pipeline.py\", line 78, in training_pipeline\n",
      "    logger.error(\"Error in training_pipeline: \", e)\n",
      "Message: 'Error in training_pipeline: '\n",
      "Arguments: (StepInterfaceError(\"Unable to unpack step artifact. This error is probably because you're trying to unpack the return value of your step but the step only returns a single artifact. For more information on how to add type annotations to your step to indicate multiple artifacts visit https://docs.zenml.io/how-to/build-pipelines/step-output-typing-and-annotation#type-annotations.\"),)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered new version: \u001b[0m\u001b[1;36m(version 32)\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mExecuting a new run.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mwandb_stack\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  experiment_tracker: \u001b[0m\u001b[1;36mwandb_experiment_tracker\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml up\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mhp_tuning\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStarting hp_tuning step...\u001b[0m\n",
      "\u001b[1;35mModelltyp: random_forest\u001b[0m\n",
      "\u001b[1;35mAnzahl der Trials: 5\u001b[0m\n",
      "\u001b[1;35mForm von X_train: (11108, 70)\u001b[0m\n",
      "\u001b[1;35mForm von y_train: (11108,)\u001b[0m\n",
      "\u001b[1;35mDatentyp von y_train ist int64\u001b[0m\n",
      "\u001b[1;35mDatentyp von y_train.iloc[0] ist 697\u001b[0m\n",
      "[I 2024-07-13 16:37:11,055] A new study created in memory with name: no-name-eae0e51a-508e-489d-b2d3-ceeeda7928e8\n",
      "[I 2024-07-13 16:37:43,714] Trial 0 finished with value: 168.1244969016035 and parameters: {'n_estimators': 123, 'max_depth': 24, 'min_samples_split': 14, 'min_samples_leaf': 3}. Best is trial 0 with value: 168.1244969016035.\n",
      "[I 2024-07-13 16:37:56,040] Trial 1 finished with value: 178.30220950552655 and parameters: {'n_estimators': 72, 'max_depth': 14, 'min_samples_split': 13, 'min_samples_leaf': 17}. Best is trial 0 with value: 168.1244969016035.\n",
      "[I 2024-07-13 16:38:07,380] Trial 2 finished with value: 178.16106431238404 and parameters: {'n_estimators': 79, 'max_depth': 8, 'min_samples_split': 14, 'min_samples_leaf': 10}. Best is trial 0 with value: 168.1244969016035.\n",
      "[I 2024-07-13 16:38:17,276] Trial 3 finished with value: 170.8388855183539 and parameters: {'n_estimators': 56, 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 7}. Best is trial 0 with value: 168.1244969016035.\n",
      "[I 2024-07-13 16:38:37,452] Trial 4 finished with value: 181.7489449888552 and parameters: {'n_estimators': 123, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 20}. Best is trial 0 with value: 168.1244969016035.\n",
      "\u001b[1;35mBest Hyperparameters: {'n_estimators': 123, 'max_depth': 24, 'min_samples_split': 14, 'min_samples_leaf': 3}\u001b[0m\n",
      "\u001b[1;35mHp-Tuning step successfully completed.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mhp_tuning\u001b[1;35m has finished in \u001b[0m\u001b[1;36m1m26s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mhp_tuning\u001b[1;35m completed successfully.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_trainer\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_trainer\u001b[1;35m has finished in \u001b[0m\u001b[1;36m43.513s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_trainer\u001b[1;35m completed successfully.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStarting evaluate_model step...\u001b[0m\n",
      "\u001b[1;35mDeployment decision: False\u001b[0m\n",
      "\u001b[1;35mFinished evaluate_model step.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.575s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m completed successfully.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m2m11s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mTraining pipeline completed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Set the model variant and type\n",
    "#m_variant = \"uni\"\n",
    "m_variant = \"multi\"\n",
    "#m_type = \"xgboost\"\n",
    "m_type = \"random_forest\"\n",
    "\n",
    "run_pipelines(model_variant=m_variant, model_type=m_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD !python run.py\n",
    "\n",
    "#import subprocess\n",
    "\n",
    "# Define the parameters\n",
    "#model_variant = \"multi\"  # Replace with \"multi\" or \"uni\"\n",
    "#model_type = \"xgb\"  # Replace with \"xgb\" or \"arima\"\n",
    "\n",
    "# Call run.py with parameters using subprocess\n",
    "#subprocess.run(['python', 'run.py', '--model_variant', model_variant, '--model_type', model_type])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "# from zenml.client import Client\n",
    "\n",
    "# client = Client()\n",
    "# #mit \n",
    "# report = client.get_artifact_version(\"\").load()\n",
    "\n",
    "# display(HTML(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nun erst die inference = Using the best model????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
